{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jit(), for speeding up your code    \n",
    "grad(), for taking derivatives   \n",
    "vmap(), for automatic vectorization or batching.   \n",
    "\n",
    "JAX vs. NumPy\n",
    "+ JAX provides a NumPy-inspired interface for convenience.\n",
    "+ Through duck-typing, JAX arrays can often be used as drop-in replacements of NumPy arrays.\n",
    "+ Unlike NumPy arrays, JAX arrays are always immutable.\n",
    "\n",
    "NumPy, lax & XLA: JAX API layering\n",
    "+ jax.numpy is a high-level wrapper that provides a familiar interface.\n",
    "+ jax.lax is a lower-level API that is stricter and often more powerful.\n",
    "+ All JAX operations are implemented in terms of operations in XLA – the Accelerated Linear Algebra compiler.\n",
    "\n",
    "To JIT or not to JIT\n",
    "+ By default JAX executes operations one at a time, in sequence.\n",
    "+ Using a just-in-time (JIT) compilation decorator, sequences of operations can be optimized together and run at once.\n",
    "+ Not all JAX code can be JIT compiled, as it requires array shapes to be static & known at compile time.\n",
    "\n",
    "JIT mechanics: tracing and static variables\n",
    "+ JIT and other JAX transforms work by tracing a function to determine its effect on inputs of a specific shape and type.\n",
    "+ Variables that you don’t want to be traced can be marked as static\n",
    "\n",
    "Static vs Traced Operations\n",
    "+ Just as values can be either static or traced, operations can be static or traced.\n",
    "+ Static operations are evaluated at compile-time in Python; traced operations are compiled & evaluated at run-time in XLA.\n",
    "+ Use numpy for operations that you want to be static; use jax.numpy for operations that you want to be traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "from jax import lax\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 ms ± 5.93 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "222 ms ± 13.9 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit -n10 -r3 jnp.dot(x, x.T).block_until_ready()  \n",
    "\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit -n10 -r3 jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.28 ms ± 788 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 ms ± 207 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661197 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x):\n",
    "    eps = 1e-3\n",
    "    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                      for v in jnp.eye(len(x))])\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0353256\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "\n",
    "def apply_matrix(v):\n",
    "    return jnp.dot(mat, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "1.43 ms ± 85.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "15.1 µs ± 351 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "  return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "34.3 µs ± 1.86 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "  return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1., 3., 4., 4., 4., 4., 4., 4., 4., 4., 3., 1.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([1, 2, 1])\n",
    "y = jnp.ones(10)\n",
    "# jnp.convolve(x, y)\n",
    "\n",
    "result = lax.conv_general_dilated(\n",
    "    x.reshape(1, 1, 3).astype(float),  # note: explicit promotion\n",
    "    y.reshape(1, 1, 10),\n",
    "    window_strides=(1, ),\n",
    "    padding=[(len(y) - 1, len(y) - 1)\n",
    "             ])  # equivalent of padding='full' in NumPy\n",
    "result[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644 µs ± 47.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "547 µs ± 57.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def norm(X):\n",
    "    X = X - X.mean(0)\n",
    "    return X / X.std(0)\n",
    "\n",
    "norm_compiled = jit(norm)\n",
    "np.random.seed(1701)\n",
    "X = jnp.array(np.random.rand(10000, 10))\n",
    "np.allclose(norm(X), norm_compiled(X), atol=1E-6)\n",
    "\n",
    "%timeit norm(X).block_until_ready()\n",
    "%timeit norm_compiled(X).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.10570311, -0.59403396, -0.8680282 , -0.23489487], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_negatives(x):\n",
    "    return x[x < 0]\n",
    "\n",
    "x = jnp.array(np.random.randn(10))\n",
    "get_negatives(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error jit 数组不能变化（编译时确定）\n",
    "# jit(get_negatives)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running f():\n",
      "  x = Traced<ShapedArray(float32[3,4])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  y = Traced<ShapedArray(float32[4])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  result = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([0.25773212, 5.3623195 , 5.403243  ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def f(x, y):\n",
    "    print(\"Running f():\")\n",
    "    print(f\"  x = {x}\")\n",
    "    print(f\"  y = {y}\")\n",
    "    result = jnp.dot(x + 1, y + 1)\n",
    "    print(f\"  result = {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "x = np.random.randn(3, 4)\n",
    "y = np.random.randn(4)\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3,4] b:f32[4]. let\n",
       "    c:f32[3,4] = add a 1.0\n",
       "    d:f32[4] = add b 1.0\n",
       "    e:f32[3] = dot_general[dimension_numbers=(([1], [0]), ([], []))] c d\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import make_jaxpr\n",
    "\n",
    "def f(x, y):\n",
    "    return jnp.dot(x + 1, y + 1)\n",
    "\n",
    "make_jaxpr(f)(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(-1, dtype=int32, weak_type=True), Array(1, dtype=int32, weak_type=True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jit, static_argnums=(1, )) # 标记静态参数\n",
    "def f(x, neg):\n",
    "    return -x if neg else x\n",
    "\n",
    "f(1, True), f(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.24124517, -1.2571421 , -0.48511598, -0.9863928 ,  1.3978302 ,\n",
       "        0.48784977,  1.9099641 , -0.26037157, -0.49505737,  1.3445066 ,\n",
       "        0.5942803 ,  0.61083764], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @jit\n",
    "# def f(x):\n",
    "#     return x.reshape(jnp.array(x.shape).prod())\n",
    "\n",
    "\n",
    "# x = jnp.ones((2, 3))\n",
    "# f(x)\n",
    "\n",
    "@jit\n",
    "def f(x):\n",
    "    return x.reshape((np.prod(x.shape), ))\n",
    "\n",
    "\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing function\n",
      "First call:  4.0\n",
      "Second call:  5.0\n",
      "Executing function\n",
      "Third call, different type:  [5.]\n"
     ]
    }
   ],
   "source": [
    "def impure_print_side_effect(x):\n",
    "    print(\"Executing function\")  # This is a side-effect\n",
    "    return x\n",
    "\n",
    "\n",
    "# The side-effects appear during the first run\n",
    "print(\"First call: \", jit(impure_print_side_effect)(4.))\n",
    "\n",
    "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
    "# This is because JAX now invokes a cached compilation of the function\n",
    "print(\"Second call: \", jit(impure_print_side_effect)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "print(\"Third call, different type: \",\n",
    "      jit(impure_print_side_effect)(jnp.array([5.])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:  4.0\n",
      "Second call:  5.0\n",
      "Third call, different type:  [14.]\n"
     ]
    }
   ],
   "source": [
    "g = 0.\n",
    "\n",
    "\n",
    "def impure_uses_globals(x):\n",
    "    return x + g\n",
    "\n",
    "\n",
    "# JAX captures the value of the global during the first run\n",
    "print(\"First call: \", jit(impure_uses_globals)(4.))\n",
    "g = 10.  # Update the global\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print(\"Second call: \", jit(impure_uses_globals)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print(\"Third call, different type: \",\n",
    "      jit(impure_uses_globals)(jnp.array([4.])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:  4.0\n",
      "Saved global:  Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "g = 0.\n",
    "\n",
    "\n",
    "def impure_saves_global(x):\n",
    "    global g\n",
    "    g = x\n",
    "    return x\n",
    "\n",
    "\n",
    "# JAX runs once the transformed function with special Traced values for arguments\n",
    "print(\"First call: \", jit(impure_saves_global)(4.))\n",
    "print(\"Saved global: \", g)  # Saved global has an internal JAX value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "def pure_uses_internal_state(x):\n",
    "    state = dict(even=0, odd=0)\n",
    "    for i in range(10):\n",
    "        state['even' if i % 2 == 0 else 'odd'] += x\n",
    "    return state['even'] + state['odd']\n",
    "\n",
    "\n",
    "print(jit(pure_uses_internal_state)(5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-34.,  nan], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def f(x, y):\n",
    "    a = x * y\n",
    "    b = (x + y) / (x - y)\n",
    "    c = a + 2\n",
    "    return a + b * c\n",
    "\n",
    "\n",
    "x = jnp.array([2., 0.])\n",
    "y = jnp.array([3., 0.])\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random.uniform(random.PRNGKey(0), (1000, ), dtype=jnp.float64)\n",
    "x.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
